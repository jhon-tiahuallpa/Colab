{"cells":[{"cell_type":"markdown","metadata":{"id":"YBrHLedGofcg"},"source":["# Qué es Machine Learning?"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"xsFFC_kFofch"},"source":["Antes de echar un vistazo a los detalles de varios métodos de aprendizaje automático, comencemos por ver qué es y qué no es el aprendizaje automático.\n","El aprendizaje automático a menudo se clasifica como un subcampo de la inteligencia artificial, pero creo que la categorización a menudo puede ser engañosa a primera vista.\n","El estudio del aprendizaje automático sin duda surgió de la investigación en este contexto, pero en la aplicación de la ciencia de datos de los métodos de aprendizaje automático, es más útil pensar en el aprendizaje automático como un medio para *construir modelos de datos*.\n","Fundamentalmente, el aprendizaje automático implica la construcción de modelos matemáticos para ayudar a comprender los datos.\n","El \"aprendizaje\" entra en juego cuando damos a estos modelos *parámetros ajustables* que se pueden adaptar a los datos observados; de esta manera se puede considerar que el programa está \"aprendiendo\" de los datos.\n","Una vez que estos modelos se han ajustado a los datos vistos anteriormente, se pueden usar para predecir y comprender aspectos de los datos recién observados.\n","Dejaré al lector la digresión más filosófica sobre hasta qué punto este tipo de \"aprendizaje\" matemático basado en modelos es similar al \"aprendizaje\" exhibido por el cerebro humano.\n","Comprender la configuración del problema en el aprendizaje automático es esencial para usar estas herramientas de manera efectiva, por lo que comenzaremos con algunas categorizaciones amplias de los tipos de enfoques que analizaremos aquí."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"I5n3WEw5ofci"},"source":["## Categorías de aprendizaje automático\n","En el nivel más fundamental, el aprendizaje automático se puede clasificar en dos tipos principales: aprendizaje supervisado y aprendizaje no supervisado.\n","*Aprendizaje supervisado* implica modelar de alguna manera la relación entre las características medidas de los datos y alguna etiqueta asociada con los datos; una vez que se determina este modelo, se puede usar para aplicar etiquetas a datos nuevos y desconocidos.\n","Esto se subdivide en tareas de *clasificación* y tareas de *regresión*: en la clasificación, las etiquetas son categorías discretas, mientras que en la regresión, las etiquetas son cantidades continuas.\n","Veremos ejemplos de ambos tipos de aprendizaje supervisado en la siguiente sección.\n","El *aprendizaje no supervisado* implica modelar las características de un conjunto de datos sin hacer referencia a ninguna etiqueta y, a menudo, se describe como \"dejar que el conjunto de datos hable por sí mismo\".\n","Estos modelos incluyen tareas como *agrupamiento* y *reducción de dimensionalidad.*\n","Los algoritmos de agrupamiento identifican distintos grupos de datos, mientras que los algoritmos de reducción de dimensionalidad buscan representaciones más sucintas de los datos.\n","Veremos ejemplos de ambos tipos de aprendizaje no supervisado en la siguiente sección.\n","Además, existen los llamados métodos de *aprendizaje semisupervisado*, que se encuentran en algún lugar entre el aprendizaje supervisado y el aprendizaje no supervisado.\n","Los métodos de aprendizaje semisupervisados ​​suelen ser útiles cuando solo se dispone de etiquetas incompletas."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"9PD5uI57ofcj"},"source":["## Ejemplos cualitativos de aplicaciones de aprendizaje automático\n","Para hacer estas ideas más concretas, echemos un vistazo a algunos ejemplos muy simples de una tarea de aprendizaje automático.\n","Estos ejemplos están destinados a brindar una descripción general intuitiva y no cuantitativa de los tipos de tareas de aprendizaje automático que veremos en este capítulo.\n","En secciones posteriores, profundizaremos en los modelos particulares y cómo se utilizan.\n","Para obtener una vista previa de estos aspectos más técnicos, puede encontrar la fuente de Python que genera las siguientes cifras en [Apéndice: Código de figuras](06.00-Código-de-figuras.ipynb).\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"L56Ak4VIofcl"},"source":["### Clasificación: predicción de etiquetas discretas\n","Primero veremos una tarea simple de *clasificación*, en la que se le proporciona un conjunto de puntos etiquetados y desea utilizarlos para clasificar algunos puntos no etiquetados.\n","Imagina que tenemos los datos que se muestran en esta figura:### Classification: Predicting discrete labels\n","\n","We will first take a look at a simple *classification* task, in which you are given a set of labeled points and want to use these to classify some unlabeled points.\n","\n","Imagine that we have the data shown in this figure:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"_p4AAkGqofcm"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-classification-1.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Classification-Example-Figure-1)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"aLwfhX3Yofcn"},"source":["Aquí tenemos datos bidimensionales: es decir, tenemos dos *características* para cada punto, representadas por las posiciones *(x,y)* de los puntos en el plano.\n","Además, tenemos una de dos *etiquetas de clase* para cada punto, aquí representada por los colores de los puntos.\n","A partir de estas características y etiquetas, nos gustaría crear un modelo que nos permita decidir si un nuevo punto debe etiquetarse como \"azul\" o \"rojo\".\n","Hay varios modelos posibles para tal tarea de clasificación, pero aquí usaremos uno extremadamente simple. Haremos la suposición de que los dos grupos se pueden separar dibujando una línea recta a través del plano entre ellos, de modo que los puntos a cada lado de la línea caigan en el mismo grupo.\n","Aquí, el *modelo* es una versión cuantitativa de la declaración \"una línea recta separa las clases\", mientras que los *parámetros del modelo* son los números particulares que describen la ubicación y la orientación de esa línea para nuestros datos.\n","Los valores óptimos para estos parámetros del modelo se aprenden de los datos (este es el \"aprendizaje\" en el aprendizaje automático), que a menudo se denomina *entrenamiento del modelo*.\n","La siguiente figura muestra una representación visual de cómo se ve el modelo entrenado para estos datos:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"mxkLHIQQofco"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-classification-2.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Classification-Example-Figure-2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Gn-CMbxVofco"},"source":["Ahora que se ha entrenado este modelo, se puede generalizar a datos nuevos sin etiquetar.\n","En otras palabras, podemos tomar un nuevo conjunto de datos, dibujar esta línea de modelo a través de él y asignar etiquetas a los nuevos puntos en función de este modelo.\n","Esta etapa se suele llamar *predicción*. Ver la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"rV8BXAtvofcp"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-classification-3.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Classification-Example-Figure-3)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"9ARacKqnofcp"},"source":["Esta es la idea básica de una tarea de clasificación en el aprendizaje automático, donde \"clasificación\" indica que los datos tienen etiquetas de clase discretas.\n","A primera vista, esto puede parecer bastante trivial: sería relativamente fácil mirar simplemente estos datos y trazar una línea tan discriminatoria para lograr esta clasificación.\n","Sin embargo, un beneficio del enfoque de aprendizaje automático es que puede generalizarse a conjuntos de datos mucho más grandes en muchas más dimensiones.\n","Por ejemplo, esto es similar a la tarea de detección automática de spam para correo electrónico; en este caso, podríamos usar las siguientes funciones y etiquetas:\n","\n","- *función 1*, *función 2*, etc. $\\to$ recuentos normalizados de palabras o frases importantes (\"Viagra\", \"príncipe nigeriano\", etc.)\n","- *etiqueta* $\\to$ \"spam\" o \"no spam\"\n","\n","\n","Para el conjunto de entrenamiento, estas etiquetas pueden determinarse mediante la inspección individual de una pequeña muestra representativa de correos electrónicos; para los correos electrónicos restantes, la etiqueta se determinaría utilizando el modelo.\n","Para un algoritmo de clasificación adecuadamente entrenado con suficientes funciones bien construidas (normalmente miles o millones de palabras o frases), este tipo de enfoque puede ser muy efectivo.\n","Veremos un ejemplo de dicha clasificación basada en texto en [En profundidad: Clasificación Naive Bayes](05.05-Naive-Bayes.ipynb).\n","Algunos algoritmos de clasificación importantes que analizaremos con más detalle son Gaussian Naive Bayes (consulte [In-Depth: Support Vector Machines](05.07-Support-Vector-Machines.ipynb)), máquinas de vectores de soporte (consulte [En profundidad: Clasificación Naive Bayes](05.05-Naive-Bayes.ipynb)) y clasificación aleatoria de bosques (consulte [En profundidad: Árboles de decisión y bosques aleatorios](05.08-Random-Forests.ipynb)).\n","\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"L1tPU2Saofcq"},"source":["### Regresión: predicción de etiquetas continuas\n","En contraste con las etiquetas discretas de un algoritmo de clasificación, a continuación veremos una tarea simple de *regresión* en la que las etiquetas son cantidades continuas.\n","Considere los datos que se muestran en la siguiente figura, que consta de un conjunto de puntos, cada uno con una etiqueta continua:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"_tvLusGdofcq"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-regression-1.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Regression-Example-Figure-1)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ONm7l0MIofcr"},"source":["Al igual que con el ejemplo de clasificación, tenemos datos bidimensionales: es decir, hay dos características que describen cada punto de datos.\n","El color de cada punto representa la etiqueta continua para ese punto.\n","Hay varios modelos de regresión posibles que podríamos usar para este tipo de datos, pero aquí usaremos una regresión lineal simple para predecir los puntos.\n","Este modelo de regresión lineal simple asume que si tratamos la etiqueta como una tercera dimensión espacial, podemos ajustar un plano a los datos.\n","Esta es una generalización de alto nivel del conocido problema de ajustar una línea a datos con dos coordenadas.\n","Podemos visualizar esta configuración como se muestra en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"58vxNSIxofcr"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-regression-2.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Regression-Example-Figure-2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kBJnJgfOofcs"},"source":["Observe que el plano *feature 1-feature 2* aquí es el mismo que en el gráfico bidimensional anterior; en este caso, sin embargo, hemos representado las etiquetas por el color y la posición del eje tridimensional.\n","Desde este punto de vista, parece razonable que ajustar un plano a través de estos datos tridimensionales nos permita predecir la etiqueta esperada para cualquier conjunto de parámetros de entrada.\n","Volviendo a la proyección bidimensional, cuando ajustamos dicho plano obtenemos el resultado que se muestra en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"H8u3KIbvofcs"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-regression-3.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Regression-Example-Figure-3)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"UgmjBk3Uofcs"},"source":["Este plano de ajuste nos da lo que necesitamos para predecir etiquetas para nuevos puntos.\n","Visualmente, encontramos los resultados que se muestran en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"paarrGpvofct"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-regression-4.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Regression-Example-Figure-4)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Qkg6wVHQofct"},"source":["Al igual que con el ejemplo de clasificación, esto puede parecer bastante trivial en un número reducido de dimensiones.\n","Pero el poder de estos métodos es que pueden aplicarse y evaluarse directamente en el caso de datos con muchas, muchas características.\n","Por ejemplo, esto es similar a la tarea de calcular la distancia a las galaxias observadas a través de un telescopio; en este caso, podríamos usar las siguientes características y etiquetas:\n","- *característica 1*, *característica 2*, etc. $\\to$ brillo de cada galaxia en una de varias longitudes de onda o colores\n","- *label* $\\to$ distancia o redshift de la galaxia\n","Las distancias para un pequeño número de estas galaxias podrían determinarse a través de un conjunto independiente de observaciones (generalmente más costosas).\n","Luego, las distancias a las galaxias restantes podrían estimarse utilizando un modelo de regresión adecuado, sin la necesidad de emplear la observación más costosa en todo el conjunto.\n","En los círculos de astronomía, esto se conoce como el problema del \"desplazamiento al rojo fotométrico\".\n","Algunos algoritmos de regresión importantes que analizaremos son la regresión lineal (consulte [En profundidad: Regresión lineal](05.06-Regresión-lineal.ipynb)), las máquinas de vectores de soporte (consulte [En profundidad: Máquinas de vectores de apoyo](05.07-Máquinas-de-vectores-de-apoyo.ipynb)) y la regresión de bosque aleatorio (consulte [A fondo: Árboles de decisión y Random Forests](05.08-Random-Forests.ipynb))."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"jq89drkHofcu"},"source":["### Clustering: inferir etiquetas en datos sin etiquetar\n","\n","Las ilustraciones de clasificación y regresión que acabamos de ver son ejemplos de algoritmos de aprendizaje supervisado, en los que intentamos construir un modelo que prediga etiquetas para nuevos datos.\n","El aprendizaje no supervisado involucra modelos que describen datos sin referencia a ninguna etiqueta conocida.\n","\n","Un caso común de aprendizaje no supervisado es la \"agrupación\", en la que los datos se asignan automáticamente a una cierta cantidad de grupos discretos.\n","Por ejemplo, podríamos tener algunos datos bidimensionales como los que se muestran en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Keucgb0aofcu"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-clustering-1.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"RWOkEvmeofcv"},"source":["A simple vista, es claro que cada uno de estos puntos es parte de un grupo distinto.\n","Dada esta entrada, un modelo de agrupamiento utilizará la estructura intrínseca de los datos para determinar qué puntos están relacionados.\n","Usando el muy rápido e intuitivo algoritmo *k*-means (ver [En profundidad: K-Means Clustering](05.11-K-Means.ipynb)), encontramos los grupos que se muestran en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7nIzE0IVofcx"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-clustering-2.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7J8MdlCaofcx"},"source":["*k*-means se ajusta a un modelo que consta de *k* centros de conglomerados; se supone que los centros óptimos son aquellos que minimizan la distancia de cada punto a su centro asignado.\n","Una vez más, esto puede parecer un ejercicio trivial en dos dimensiones, pero a medida que nuestros datos se vuelven más grandes y complejos, estos algoritmos de agrupamiento se pueden emplear para extraer información útil del conjunto de datos.\n","Discutiremos el algoritmo *k*-means con más profundidad en [En profundidad: K-Means Clustering](05.11-K-Means.ipynb).\n","Otros algoritmos de agrupamiento importantes incluyen modelos de mezcla gaussiana (consulte [En profundidad: Modelos de mixturas gaussianas](05.12-Mixturas-Gaussianass.ipynb)) y agrupamiento espectral [Documentación sobre clustering de Scikit-Learn](http://scikit-learn.org/stable/modules/clustering.html))."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2tTNrT-jofcx"},"source":["### Reducción de la dimensionalidad: inferir la estructura de los datos sin etiquetar\n","La reducción de la dimensionalidad es otro ejemplo de un algoritmo no supervisado, en el que las etiquetas u otra información se deducen de la estructura del propio conjunto de datos.\n","La reducción de la dimensionalidad es un poco más abstracta que los ejemplos que vimos antes, pero generalmente busca extraer alguna representación de datos de baja dimensión que de alguna manera conserva las cualidades relevantes del conjunto de datos completo.\n","Diferentes rutinas de reducción de dimensionalidad miden estas cualidades relevantes de diferentes maneras, como veremos en [A profundidad: El aprendizaje múltiple](05.10-Aprendizaje-Múltiple.ipynb).\n","Como ejemplo de esto, considere los datos que se muestran en la siguiente figura:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ku85CUoLofcy"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-dimesionality-1.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-1)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"DC0OUcQ2ofcy"},"source":["Visualmente, está claro que hay cierta estructura en estos datos: se extrae de una línea unidimensional que se organiza en espiral dentro de este espacio bidimensional.\n","En cierto sentido, se podría decir que estos datos son \"intrínsecamente\" solo unidimensionales, aunque estos datos unidimensionales están incrustados en un espacio de dimensiones superiores.\n","Un modelo de reducción de dimensionalidad adecuado en este caso sería sensible a esta estructura incrustada no lineal y podría extraer esta representación de menor dimensionalidad.\n","La siguiente figura muestra una visualización de los resultados del algoritmo Isomap, un algoritmo de aprendizaje múltiple que hace exactamente esto:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"GiUJUregofcy"},"source":["![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.01-dimesionality-2.png?raw=1)\n","[fuente de la figura en el Apéndice](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"K07l2oEtofcy"},"source":["Observe que los colores (que representan la variable latente unidimensional extraída) cambian uniformemente a lo largo de la espiral, lo que indica que el algoritmo de hecho detectó la estructura que vimos a simple vista.\n","Al igual que con los ejemplos anteriores, el poder de los algoritmos de reducción de dimensionalidad se vuelve más claro en casos de dimensiones superiores.\n","\n","Por ejemplo, podríamos desear visualizar relaciones importantes dentro de un conjunto de datos que tiene 100 o 1000 características.\n","\n","Visualizar datos de 1000 dimensiones es un desafío, y una forma en que podemos hacer que esto sea más manejable es usar una técnica de reducción de dimensionalidad para reducir los datos a dos o tres dimensiones.\n","Algunos algoritmos de reducción de dimensionalidad importantes que analizaremos son el análisis de componentes principales (consulte [En profundidad: Análisis de componentes principales](05.09-Análisis-de-componentes-principales.ipynb)) y varios algoritmos de aprendizaje múltiple, incluidos Isomap y la incrustación lineal local (consulte [A profundidad: El aprendizaje múltiple](05.10-Aprendizaje-Múltiple.ipynb)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"vBJk80-Uofcz"},"source":["## Resumen\n","Aquí hemos visto algunos ejemplos simples de algunos de los tipos básicos de enfoques de aprendizaje automático.\n","No hace falta decir que hay una serie de detalles prácticos importantes que hemos pasado por alto, pero espero que esta sección haya sido suficiente para darle una idea básica de qué tipos de problemas pueden resolver los enfoques de aprendizaje automático.\n","En resumen, vimos lo siguiente:\n","- *Aprendizaje supervisado*: modelos que pueden predecir etiquetas en función de los datos de entrenamiento etiquetados\n","    - *Clasificación*: Modelos que predicen etiquetas como dos o más categorías discretas\n","    - *Regresión*: Modelos que predicen etiquetas continuas\n","\n","- *Aprendizaje no supervisado*: Modelos que identifican estructura en datos no etiquetados\n","    - *Clustering*: Modelos que detectan e identifican distintos grupos en los datos\n","    - *Reducción de dimensionalidad*: Modelos que detectan e identifican estructuras de menor dimensión en datos de mayor dimensión\n","\n","En las siguientes secciones profundizaremos mucho más en estas categorías y veremos algunos ejemplos más interesantes de dónde pueden ser útiles estos conceptos.\n","Todas las cifras de la discusión anterior se generan en base a cálculos reales de aprendizaje automático; el código detrás de ellos se puede encontrar en [Apéndice: Código de figuras](06.00-Código de figuras.ipynb)."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"97gYeCRgofcz"},"source":["<!--NAVIGATION-->\n","< [Machine Learning](05.00-Machine-Learning.ipynb) | [Contents](Index.ipynb) | [Introducing Scikit-Learn](05.02-Introducing-Scikit-Learn.ipynb) >\n","\n","<a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.1"},"colab":{"provenance":[{"file_id":"https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb","timestamp":1661916351601}],"name":"05.01-Qué-es-Machine-Learning.ipynb","collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}